# makemore

This repository contains implementations of character-level language models. Two character level language models are implemented to generate baby names using the *names.txt* training dataset. Another character level language model is implemented to generate creative Shakespeare-like text using *tinyshakespeare/input.txt* as the training dataset.

The project covers the implementation of a multilayer perceptron (MLP), a WaveNet-style model, and a Transformer model for generating text at the character level. The implementations for these are written from scratch in Python. I create my own mini version of PyTorch in the process writing the base classes for these architectures from scratch.

## Motivation
Large language models have become a fundamental part of modern-day AI research and Natural Language Processing. This project was inspired by Andrej Karpathyâ€™s makemore series and aims to build a deep understanding of how these models function from the ground up.

Rather than relying on existing deep learning frameworks, I implemented the core building blocks of MLPs, WaveNet, and Transformer models from scratch, creating a mini version of PyTorch along the way. This approach provided hands-on experience with backpropagation, optimization techniques, model training and evaluation and architectural innovations like batch normalization, dropout, causal convolutions, and self-attention.

Through this project, I aimed to develop a strong intuition for designing and debugging deep learning models while deepening my understanding of low-level neural network implementations.


## Features
- Multilayer Perceptron (MLP) Model: A deep neural network trained on character sequences with feedforward linear layers. Implementation of the seminal paper by Bengio et al.: https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf

- WaveNet-Inspired Model: A convolutional model with hierarchical feature extraction. Implementation of the seminal paper by Google DeepMind: https://arxiv.org/abs/1609.03499

- Transformer Model: An implementation of the GPT-2 architecture with multiple self attention heads, layer norm, and dropout. Implementation of decoder-only version by Vaswani et al.: https://arxiv.org/abs/1706.03762. Also implements ideas from OpenAI GPT-3 release: https://arxiv.org/abs/2005.14165

## Files Overview

- minitorch.py - Core implementation details for the MLP, WaveNet, and Transformer models written from scratch to create a custom mini version of PyTorch.

- mlp.ipynb - Jupyter notebook for training and evaluating the MLP.

- wavenet.ipynb - Jupyter notebook for training and evaluating the WaveNet model.

- gpt2.ipynb - Jupyter notebook for training and evaluating the GPT-2 type transformer model.

- names.txt - Sample dataset of names used for training.

## Results
Below are some sample outputs generated by the models trained on different datasets.

**MLP - Baby Names Generation**

daily.
barbesta.
da.
jasin.
rirnola.
lovelle.
emerson.
ana.

**WaveNet - Baby Names Generation**

rileas.
adel.
alexxis.
shamira.
tahari.
selena.
nayeli.
noel.


**GPT - Creative Shakespeare**

CLARENCE:
Nay, sir, that march well may to-man, would,
And we letter Name go all, I ready tiele,
And, winning the cursh out oft your uping soul.

CLARENCE:
Why, for we would you stay ship you lets us conce;
Or, beard Witchmand and bear thy tongue of their
As thou drun. If you answer for knower; but child non
The unlovider Yand be none mean the mind years.
MERCK:
My greater Pomper forth, lords,
What you woy may Pribhal me about,
Who must aven you go fourtner. What, mift you
Were god in so the cursess:  of a trunne worsed
Shall of whom. My come, if you I do you
Voul us, this judge; book your me; but your greath
A ser
