{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Set Up\n",
        "\n",
        "- load data\n",
        "- create character level tokens (transform into integers);\n",
        "- create train / test splits\n"
      ],
      "metadata": {
        "id": "zSXSs34rQF0l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kOf3jLitOU75",
        "outputId": "e6b3dba0-5cc0-4c4a-bf14-98dc7ccacd26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-17 10:08:45--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2025-02-17 10:08:45 (101 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"length of dataset in characters: \", len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OmmKUwnSPEr0",
        "outputId": "d7ddbe73-1e9c-4ead-8995-c574a6d204f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters:  1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's look at the first 1000 characters\n",
        "print(text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFLQ-2TDPRCF",
        "outputId": "517e0151-a57d-45f3-840d-a3fd4e6ef83c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(\"Vocab Size:\", vocab_size)\n",
        "\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "\n",
        "\n",
        "encoder = lambda s: [stoi[c] for c in s]\n",
        "decoder = lambda l: ''.join([itos[i] for i in l])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZ6NzbmSPSqC",
        "outputId": "acfaa576-0044-441d-cb34-0f7a0e8f10de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab Size: 65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = torch.tensor(encoder(text), dtype=torch.long)\n",
        "n = int(0.9 * len(data))\n",
        "train = data[:n]\n",
        "val = data[n:]\n",
        "train.shape, val.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jV9ZFeaBQC8w",
        "outputId": "49e1ab25-8428-4fe1-a26c-4a062d8c985c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1003854]), torch.Size([111540]))"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# intuition behind data preparation\n",
        "block_size=8\n",
        "x = train[:block_size]\n",
        "y = train[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    print(f\"{t}/{block_size} when input is {context} the target: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMI1jXGfSqAT",
        "outputId": "a5c8b48f-d30f-441c-aaea-bbef6f16ea77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0/8 when input is tensor([18]) the target: 47\n",
            "1/8 when input is tensor([18, 47]) the target: 56\n",
            "2/8 when input is tensor([18, 47, 56]) the target: 57\n",
            "3/8 when input is tensor([18, 47, 56, 57]) the target: 58\n",
            "4/8 when input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
            "5/8 when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
            "6/8 when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
            "7/8 when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper Functions"
      ],
      "metadata": {
        "id": "qzN5FEescxhf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get a small randomly sampled batch of data\n",
        "def get_batch(split):\n",
        "  data = train if split == 'train' else val\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "  x, y = x.to(device), y.to(device)\n",
        "  return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "  out = {}\n",
        "  model.eval()\n",
        "  for split in ['train', 'val']:\n",
        "    losses = torch.zeros(eval_iters)\n",
        "    for k in range(eval_iters):\n",
        "      X, Y = get_batch(split)\n",
        "      logits, loss = model(X, Y)\n",
        "      losses[k] = loss.item()\n",
        "    out[split] = losses.mean()\n",
        "  model.train()\n",
        "  return out"
      ],
      "metadata": {
        "id": "Sxy1PvKAQEJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer Components"
      ],
      "metadata": {
        "id": "F3uQCYBwczKY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttentionHead(nn.Module):\n",
        "  \"\"\" one self attention head \"\"\"\n",
        "  def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "    self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, T, C = x.shape\n",
        "    k = self.key(x)\n",
        "    q = self.query(x)\n",
        "\n",
        "    # compute attention scores / affinities between words\n",
        "    wei = q @ k.transpose(-2, -1) * C**-0.5\n",
        "    wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "    wei = F.softmax(wei, dim=-1)\n",
        "    wei = self.dropout(wei)\n",
        "\n",
        "    # weighted aggregation of values\n",
        "    v = self.value(x)\n",
        "    out = wei @ v\n",
        "    return out\n",
        "\n",
        "class MultiHead(nn.Module):\n",
        "  \"\"\" multiple self attention heads concatenated \"\"\"\n",
        "  def __init__(self, num_heads, head_size):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([SelfAttentionHead(head_size) for _ in range(num_heads)])\n",
        "    self.projection = nn.Linear(n_embd, n_embd)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "    out = self.projection(out)\n",
        "    out = self.dropout(out)\n",
        "    return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, n_embd):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "      # n_embd is the input size, 4*n_embd is the hidden dimension size\n",
        "      nn.Linear(n_embd, 4 * n_embd),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(4 * n_embd, n_embd),\n",
        "      nn.Dropout(dropout)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.net(x)\n",
        "    return out\n",
        "\n",
        "class LayerNorm1d(nn.Module):\n",
        "  def __init__(self, dim, eps=1e-5):\n",
        "    super().__init__()\n",
        "    self.eps = eps\n",
        "    self.gamma = torch.ones(dim)\n",
        "    self.beta = torch.zeros(dim)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    xmean = x.mean(dim=1, keepdim=True)\n",
        "    xvar = x.var(dim=1, keepdim=True)\n",
        "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps)\n",
        "    self.out = self.gamma * xhat + self.beta\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.gamma, self.beta]\n",
        "\n",
        "class Block(nn.Module):\n",
        "  def __init__(self, n_embd, n_head):\n",
        "    super().__init__()\n",
        "    head_size = n_embd // n_head\n",
        "    self.a = MultiHead(n_head, head_size)\n",
        "    self.ff = FeedForward(n_embd)\n",
        "    self.ln1 = nn.LayerNorm(n_embd)\n",
        "    self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # use of residual connections\n",
        "    x = x + self.a(self.ln1(x))\n",
        "    x = x + self.ff(self.ln2(x))\n",
        "    return x\n",
        "\n",
        "class LanguageModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "    self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "    self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "    self.ln = nn.LayerNorm(n_embd)\n",
        "    self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    B, T = idx.shape\n",
        "\n",
        "    tok_emb = self.token_embedding_table(idx)\n",
        "    pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "\n",
        "    x = tok_emb + pos_emb\n",
        "    x = self.blocks(x)\n",
        "    x = self.ln(x)\n",
        "    logits = self.lm_head(x)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    # idx is (B, T) array of indices in the current context\n",
        "    for _ in range(max_new_tokens):\n",
        "      # crop idx to the last block_size tokens\n",
        "      idx_cond = idx[:, -block_size:]\n",
        "      # get the predictions\n",
        "      logits, loss = self(idx_cond)\n",
        "      # focus only on the last time step\n",
        "      logits = logits[:, -1, :] # becomes (B, C)\n",
        "      # apply softmax to get probabilities\n",
        "      probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "      # sample from the distribution\n",
        "      idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "      # append sampled index to the running sequence\n",
        "      idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "    return idx\n"
      ],
      "metadata": {
        "id": "877Vle7kVYZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "645HjxR9c2Pt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#######################\n",
        "# Hyperparameters\n",
        "device = \"cuda\" if torch.cuda.is_available()else \"cpu\"\n",
        "eval_iters = 200\n",
        "max_iters = 7000\n",
        "learning_rate = 1e-4\n",
        "\n",
        "batch_size = 64\n",
        "block_size = 256\n",
        "\n",
        "n_head = 6\n",
        "n_layer = 6\n",
        "n_embd = 384\n",
        "dropout = 0.2\n",
        "\n",
        "\n",
        "#######################\n",
        "\n",
        "model = LanguageModel()\n",
        "model = model.to(device)\n",
        "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "zX2Mlr9YbB4N",
        "outputId": "2ad81d47-5d0a-4b05-81e4-6abe3b1ce7b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10.788929 M parameters\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LanguageModel(\n",
              "  (token_embedding_table): Embedding(65, 384)\n",
              "  (position_embedding_table): Embedding(256, 384)\n",
              "  (blocks): Sequential(\n",
              "    (0): Block(\n",
              "      (a): MultiHead(\n",
              "        (heads): ModuleList(\n",
              "          (0-5): 6 x SelfAttentionHead(\n",
              "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (projection): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (1): Block(\n",
              "      (a): MultiHead(\n",
              "        (heads): ModuleList(\n",
              "          (0-5): 6 x SelfAttentionHead(\n",
              "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (projection): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (2): Block(\n",
              "      (a): MultiHead(\n",
              "        (heads): ModuleList(\n",
              "          (0-5): 6 x SelfAttentionHead(\n",
              "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (projection): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (3): Block(\n",
              "      (a): MultiHead(\n",
              "        (heads): ModuleList(\n",
              "          (0-5): 6 x SelfAttentionHead(\n",
              "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (projection): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (4): Block(\n",
              "      (a): MultiHead(\n",
              "        (heads): ModuleList(\n",
              "          (0-5): 6 x SelfAttentionHead(\n",
              "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (projection): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (5): Block(\n",
              "      (a): MultiHead(\n",
              "        (heads): ModuleList(\n",
              "          (0-5): 6 x SelfAttentionHead(\n",
              "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (projection): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (ln): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "  (lm_head): Linear(in_features=384, out_features=65, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "for iter in range(max_iters):\n",
        "  if iter % eval_iters == 0 or iter == max_iters - 1:\n",
        "    losses = estimate_loss()\n",
        "    print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "  xb, yb = get_batch(\"train\")\n",
        "\n",
        "  logits, loss = model(xb, yb)\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "c-1lFe-1cnKD",
        "outputId": "8462ed05-488b-45ee-bfa6-143f9e5793fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.2849, val loss 4.2824\n",
            "step 200: train loss 2.4768, val loss 2.4906\n",
            "step 400: train loss 2.4244, val loss 2.4494\n",
            "step 600: train loss 2.3505, val loss 2.3714\n",
            "step 800: train loss 2.2024, val loss 2.2424\n",
            "step 1000: train loss 2.0857, val loss 2.1438\n",
            "step 1200: train loss 1.9891, val loss 2.0715\n",
            "step 1400: train loss 1.9080, val loss 2.0078\n",
            "step 1600: train loss 1.8379, val loss 1.9609\n",
            "step 1800: train loss 1.7799, val loss 1.9242\n",
            "step 2000: train loss 1.7287, val loss 1.8833\n",
            "step 2200: train loss 1.6789, val loss 1.8388\n",
            "step 2400: train loss 1.6370, val loss 1.8155\n",
            "step 2600: train loss 1.5999, val loss 1.7838\n",
            "step 2800: train loss 1.5722, val loss 1.7551\n",
            "step 3000: train loss 1.5414, val loss 1.7315\n",
            "step 3200: train loss 1.5252, val loss 1.7199\n",
            "step 3400: train loss 1.4982, val loss 1.6966\n",
            "step 3600: train loss 1.4756, val loss 1.6792\n",
            "step 3800: train loss 1.4589, val loss 1.6636\n",
            "step 4000: train loss 1.4389, val loss 1.6425\n",
            "step 4200: train loss 1.4252, val loss 1.6396\n",
            "step 4400: train loss 1.4047, val loss 1.6174\n",
            "step 4600: train loss 1.3989, val loss 1.6149\n",
            "step 4800: train loss 1.3846, val loss 1.6051\n",
            "step 5000: train loss 1.3718, val loss 1.5945\n",
            "step 5200: train loss 1.3535, val loss 1.5799\n",
            "step 5400: train loss 1.3462, val loss 1.5755\n",
            "step 5600: train loss 1.3381, val loss 1.5691\n",
            "step 5800: train loss 1.3264, val loss 1.5610\n",
            "step 6000: train loss 1.3158, val loss 1.5562\n",
            "step 6200: train loss 1.3059, val loss 1.5414\n",
            "step 6400: train loss 1.2995, val loss 1.5387\n",
            "step 6600: train loss 1.2897, val loss 1.5359\n",
            "step 6800: train loss 1.2801, val loss 1.5320\n",
            "step 7000: train loss 1.2757, val loss 1.5315\n",
            "step 7200: train loss 1.2655, val loss 1.5155\n",
            "step 7400: train loss 1.2563, val loss 1.5207\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-ea18d6cbf003>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-4ee5aa2b0498>\u001b[0m in \u001b[0;36mget_batch\u001b[0;34m(split)\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mblock_size\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mblock_size\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decoder(model.generate(context, max_new_tokens=2000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nNYg0UhadevF",
        "outputId": "8492f43f-f992-4bd5-86d9-51b0ae7fc44e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Jount Not by love, beliked,\n",
            "Whose the tric of the doth torchee frogeth;\n",
            "Why crown to did Lawn Edglishire, and beath\n",
            "More seizards than that things royalt.\n",
            "What see tolemen? not this news, not Gaunt?\n",
            "Or shows cewards? what contured, stand with you tow;\n",
            "Wherefore, because they think it with there bruithers?\n",
            "And I plead, let's your heaving, Claudious, sir,\n",
            "Or the like is name.\n",
            "\n",
            "CLARENCE:\n",
            "Nay, sir, that march well may to-man, would,\n",
            "And we letter Name go all, I ready tiele,\n",
            "And, winning the cursh out oft your uping soul.\n",
            "\n",
            "CLARENCE:\n",
            "Why, for we would you stay ship you lets us conce;\n",
            "Or, beard Witchmand and bear thy tongue of their\n",
            "As thou drun. If you answer for knower; but child non\n",
            "The unlovider Yand be none mean the mind years.\n",
            "MERCK:\n",
            "My greater Pomper forth, lords,\n",
            "What you woy may Pribhal me about,\n",
            "Who must aven you go fourtner. What, mift you\n",
            "Were god in so the cursess:  of a trunne worsed\n",
            "Shall of whom. My come, if you I do you\n",
            "Voul us, this judge; book your me; but your greath\n",
            "A ser\n"
          ]
        }
      ]
    }
  ]
}